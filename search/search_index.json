{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Machine Learning for Particle Track Reconstruction Welcome to repository and documentation for ML pipelines and techniques by the ExatrkX Collaboration. Here we present a set of templates, best practices and results gathered from significant trial and error, to speed up the development of others in the domain of machine learning for high energy physics. We focus on applications specific to detector physics, but many tools can be applied to other areas, and these are collected in an application-agnostic way in the Tools section. Intro To start as quickly as possible, clone the repository, Install and follow the steps in Quickstart . This will get you generating toy tracking data and running inference immediately. Many of the choices of structure will be made clear there. If you already have a particle physics problem in mind, you can apply the Template that is most suitable to your use case. Once up and running, you may want to consider more complex ML Models . Many of these are built on other libraries (for example Pytorch Geometric ). Install It's recommended to start a conda environment before installation: conda install --name exatrkx-tracking python=3.8 conda activate exatrkx-tracking pip install pip --upgrade If you have a CUDA GPU available, load the toolkit or install it now. python install.py will attempt to negotiate a path through the packages required, using nvcc --version to automatically find the correct wheels. You should be ready for the Quickstart ! If this doesn't work, you can step through the process manually: CPU GPU 1. Run `export CUDA=cpu` 1a. Find the GPU version cuda XX.X with `nvcc --version` 1b. Run `export CUDA=cuXXX`, with `XXX = 92, 101, 102, 110` 2. Install Pytorch and dependencies ```pip install --user -r requirements.txt``` 3. Install local packages ```pip install -e .``` 4. Install CPU-optimized packages ```pip install faiss-cpu pip install \"git+https://github.com/facebookresearch/pytorch3d.git@stable\"``` 4. Install GPU-optimized packages ```pip install faiss-gpu cupy-cudaXXX```, with `XXX` ```pip install pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py3{Y}_cu{XXX}_pyt{ZZZ}/download.html``` where `{Y}` is the minor version of Python 3.{Y}, `{XXX}` is as above, and `{ZZZ}` is the version of Pytorch {Z.ZZ}. e.g. `py36_cu101_pyt170` is Python 3.6, Cuda 10.1, Pytorch 1.70.","title":"Home"},{"location":"#machine-learning-for-particle-track-reconstruction","text":"Welcome to repository and documentation for ML pipelines and techniques by the ExatrkX Collaboration. Here we present a set of templates, best practices and results gathered from significant trial and error, to speed up the development of others in the domain of machine learning for high energy physics. We focus on applications specific to detector physics, but many tools can be applied to other areas, and these are collected in an application-agnostic way in the Tools section.","title":"Machine Learning for Particle Track Reconstruction"},{"location":"#intro","text":"To start as quickly as possible, clone the repository, Install and follow the steps in Quickstart . This will get you generating toy tracking data and running inference immediately. Many of the choices of structure will be made clear there. If you already have a particle physics problem in mind, you can apply the Template that is most suitable to your use case. Once up and running, you may want to consider more complex ML Models . Many of these are built on other libraries (for example Pytorch Geometric ).","title":"Intro"},{"location":"#install","text":"It's recommended to start a conda environment before installation: conda install --name exatrkx-tracking python=3.8 conda activate exatrkx-tracking pip install pip --upgrade If you have a CUDA GPU available, load the toolkit or install it now. python install.py will attempt to negotiate a path through the packages required, using nvcc --version to automatically find the correct wheels. You should be ready for the Quickstart ! If this doesn't work, you can step through the process manually: CPU GPU 1. Run `export CUDA=cpu` 1a. Find the GPU version cuda XX.X with `nvcc --version` 1b. Run `export CUDA=cuXXX`, with `XXX = 92, 101, 102, 110` 2. Install Pytorch and dependencies ```pip install --user -r requirements.txt``` 3. Install local packages ```pip install -e .``` 4. Install CPU-optimized packages ```pip install faiss-cpu pip install \"git+https://github.com/facebookresearch/pytorch3d.git@stable\"``` 4. Install GPU-optimized packages ```pip install faiss-gpu cupy-cudaXXX```, with `XXX` ```pip install pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py3{Y}_cu{XXX}_pyt{ZZZ}/download.html``` where `{Y}` is the minor version of Python 3.{Y}, `{XXX}` is as above, and `{ZZZ}` is the version of Pytorch {Z.ZZ}. e.g. `py36_cu101_pyt170` is Python 3.6, Cuda 10.1, Pytorch 1.70.","title":"Install"},{"location":"pipelines/","text":"","title":"Pipelines"},{"location":"models/overview/","text":"Models for Tracking ML Including a New Model This repository aims to collect modular ML models that can be reused for various HEP applications. As such, new models are welcome to be included. Currently, new models can either be added to the general model collection in src/Architectures/ or, if the model is intended to be used in an example application, in that application's pipeline in src/Pipelines/APPLICATION/LightningModules/ . Models are organised by their architecture, which allows much of the common code to be abstracted out. For example, a particular convolution of a GNN can be specified in src/.../GNN/Models/my_new_gnn.py which inherits all the training behavior of GNNBase in GNN/gnn_base.py . The models are written in Pytorch Lightning, to avoid as much training boilerplate as possible. To include a new model, or a new base class that doesn't yet exist, it should be implemented in Pytorch Lightning. It usually takes 5-10 minutes to convert vanilla Pytorch code to Pytorch Lightning , see for example this Lightning Tutorial. Once a model is included in a Pipeline, it can included as a stage, as in the TrackML Example . Available Models Embeddings Edge MLPs Static GNNs Dynamic GNNs Top Takeaways Best model for each use case (by memory, timing, etc.) Best hyperparameter choice Test Model","title":"Overview"},{"location":"models/overview/#models-for-tracking-ml","text":"","title":"Models for Tracking ML"},{"location":"models/overview/#including-a-new-model","text":"This repository aims to collect modular ML models that can be reused for various HEP applications. As such, new models are welcome to be included. Currently, new models can either be added to the general model collection in src/Architectures/ or, if the model is intended to be used in an example application, in that application's pipeline in src/Pipelines/APPLICATION/LightningModules/ . Models are organised by their architecture, which allows much of the common code to be abstracted out. For example, a particular convolution of a GNN can be specified in src/.../GNN/Models/my_new_gnn.py which inherits all the training behavior of GNNBase in GNN/gnn_base.py . The models are written in Pytorch Lightning, to avoid as much training boilerplate as possible. To include a new model, or a new base class that doesn't yet exist, it should be implemented in Pytorch Lightning. It usually takes 5-10 minutes to convert vanilla Pytorch code to Pytorch Lightning , see for example this Lightning Tutorial. Once a model is included in a Pipeline, it can included as a stage, as in the TrackML Example .","title":"Including a New Model"},{"location":"models/overview/#available-models","text":"Embeddings Edge MLPs Static GNNs Dynamic GNNs","title":"Available Models"},{"location":"models/overview/#top-takeaways","text":"Best model for each use case (by memory, timing, etc.) Best hyperparameter choice","title":"Top Takeaways"},{"location":"models/overview/#test-model","text":"","title":"Test Model"},{"location":"models/taxonomy/","text":"ML Models Used in Tracking","title":"Zoology and Taxonomy"},{"location":"models/taxonomy/#ml-models-used-in-tracking","text":"","title":"ML Models Used in Tracking"},{"location":"performance/overview/","text":"","title":"Overview"},{"location":"pipelines/overview/","text":"Tracking ML Pipelines Define pipeline clearly Pytorch lightning & MLFlow Data processing Graph construction Graph neural network Post-processing Follow the tutorial in Quickstart to get up and running quickly with a toy model. How do Pipelines work? The aim of our pipeline structure is to abstract out as much repetitive code as possible. A pipeline is defined by a YAML config file, which only requires three inputs: The location of your model definitions model_library , the location to save/load your artifacts artifact_library , and the stages of the pipeline stage_list . An example stage list is - {set: GNN, name: SliceCheckpointedResAGNN, config: train_gnn.yaml} The set key defines the type of ML model (to help avoid naming ambiguity), the name key is the class of the model, and the config key is the file specifying your choice of hyperparameters, directories, callbacks and so on. And that's it! The repository through an application lens, using Pipelines for specific physics goals Why this choice of abstraction? I found that my two forms of R&D fell into breadth and depth. Much of the time, I would play at length with hyperparameters and model definitions, in which case I want that all to live in one place: The model's config file. Thus the pipeline config can remain untouched if we have one for each choice of (model, config) , or only changed occasionally if we choose to have only one. At other times, development would require a series of models, where successive results depend on hyperparameter choices earlier in the chain. Then I can play with the higher level pipeline config and try difference (model, config) stages, while the whole chain of hyperparameters is committed to each step via a logging platform (Weights & Biases in my case). Pytorch Lightning & MLFlow This repository uses Pytorch Lightning, which allows us to encapsulate all training and model logic into a module object. This module is what is being specified by the pipeline config, with name . Combined with callbacks in the model config file, all pipeline logic is contained in each module. A callback object integrates with the module and knows about telemetry and post-processing steps. Rather than a monolithic function that passes data through the pipeline (whereby mistakes could be made), the pipeline asks each model how it should be treated, and then acts with the model's own methods. Data Processing & Modularity Define clearly the data structure of each modular section","title":"Overview"},{"location":"pipelines/overview/#tracking-ml-pipelines","text":"Define pipeline clearly Pytorch lightning & MLFlow Data processing Graph construction Graph neural network Post-processing Follow the tutorial in Quickstart to get up and running quickly with a toy model.","title":"Tracking ML Pipelines"},{"location":"pipelines/overview/#how-do-pipelines-work","text":"The aim of our pipeline structure is to abstract out as much repetitive code as possible. A pipeline is defined by a YAML config file, which only requires three inputs: The location of your model definitions model_library , the location to save/load your artifacts artifact_library , and the stages of the pipeline stage_list . An example stage list is - {set: GNN, name: SliceCheckpointedResAGNN, config: train_gnn.yaml} The set key defines the type of ML model (to help avoid naming ambiguity), the name key is the class of the model, and the config key is the file specifying your choice of hyperparameters, directories, callbacks and so on. And that's it! The repository through an application lens, using Pipelines for specific physics goals","title":"How do Pipelines work?"},{"location":"pipelines/overview/#why-this-choice-of-abstraction","text":"I found that my two forms of R&D fell into breadth and depth. Much of the time, I would play at length with hyperparameters and model definitions, in which case I want that all to live in one place: The model's config file. Thus the pipeline config can remain untouched if we have one for each choice of (model, config) , or only changed occasionally if we choose to have only one. At other times, development would require a series of models, where successive results depend on hyperparameter choices earlier in the chain. Then I can play with the higher level pipeline config and try difference (model, config) stages, while the whole chain of hyperparameters is committed to each step via a logging platform (Weights & Biases in my case).","title":"Why this choice of abstraction?"},{"location":"pipelines/overview/#pytorch-lightning-mlflow","text":"This repository uses Pytorch Lightning, which allows us to encapsulate all training and model logic into a module object. This module is what is being specified by the pipeline config, with name . Combined with callbacks in the model config file, all pipeline logic is contained in each module. A callback object integrates with the module and knows about telemetry and post-processing steps. Rather than a monolithic function that passes data through the pipeline (whereby mistakes could be made), the pipeline asks each model how it should be treated, and then acts with the model's own methods.","title":"Pytorch Lightning &amp; MLFlow"},{"location":"pipelines/overview/#data-processing-modularity","text":"Define clearly the data structure of each modular section","title":"Data Processing &amp; Modularity"},{"location":"pipelines/quickstart/","text":"Quickstart Tutorial 1. Install See instructions at Install . 2. Get Dataset For now, let's keep our data in a directory, with a location saved to /my/data/path : export EXATRKX_DATA=/my/data/path (you can hard-code these into your custom configs later). The easiest way to get the TrackML dataset is to use the Kaggle API. Install it with pip install kaggle and grab a small toy dataset with kaggle competitions download \\ -c trackml-particle-identification \\ -f train_sample.zip \\ -p $EXATRKX_DATA 3. Running the Pipeline Configuration A pipeline runs at three layers of configuration, to allow for as much flexibility as possible. To get running immediately however, you needn't change any of the defaults. From the src/Pipelines/TrackML_Example/ directory, we run python run_pipeline.py config/pipeline_quickstart.yaml which loads the pipeline specified in config/pipeline_quickstart . While it's running, get a cup of tea and a Tim-Tam, and let's see what it's doing: Default behaviour Our quickstart pipeline is running three stages, with a single configuration for each. You can see in config/pipeline_quickstart.yaml that the three stages are: A Processing stage with the class FeatureStore and config prepare_small_feature_store.yaml ; An Embedding stage with the class LayerlessEmbedding and config train_small_embedding.yaml ; and A Filter stage with the class VanillaFilter and config train_small_filter.yaml . The Processing stage is exactly that: data processing. It is not \"trainable\", and so the pipeline treats it differently than a trainable stage. Under the hood, it is a LightningDataModule, rather than the trainable models, which inherit from LightningModule. In this case, FeatureStore is performing some calculations on the cell information in the detector, and constructing truth graphs that will later be used for training. These calculations are computationally expensive, so it doesn't make sense to calculate them on-the-fly while training. The trainable models Embedding and Filter are learning the non-linear metric of the truth graphs, and pairwise likelihoods of hits sharing a truth graph edge, respectively. The details are not so important at this stage, what matters is that these stages are modular: Each one can be run alone, but by adding a callback to the end, it can prepare the dataset for the next stage. Looking at LightningModules/Embedding/train_small_embedding.yaml you will see that a callback is given as callbacks: EmbeddingInferenceCallback . Any number of callbacks can be added, and they adhere to the Lightning callback system . The one referred to here runs the best version of the trained Embedding model on each directory of the data split (train, val, test) and saves it for the next stage. We could also add a telemetry callback, e.g. the EmbeddingTelemetry callback in LightningModules/Embedding/Models/inference.py . This callback prints a PDF of the transverse momentum vs. the efficiency of the metric learning model, saving it in the output_dir . It \"hooks\" into the testing phase, which is run after every training phase: The default settings of this run pull from the three configuration files given at each stage. You can look at them Global config: Model library: This is in the repository Artifact library: This should be a place where you can store data = checkpoints Pipeline config: explain stages in it Explain default model config parameters (1GeV, n_train=90, n_val/n_test = 5)","title":"Quickstart"},{"location":"pipelines/quickstart/#quickstart-tutorial","text":"","title":"Quickstart Tutorial"},{"location":"pipelines/quickstart/#1-install","text":"See instructions at Install .","title":"1. Install"},{"location":"pipelines/quickstart/#2-get-dataset","text":"For now, let's keep our data in a directory, with a location saved to /my/data/path : export EXATRKX_DATA=/my/data/path (you can hard-code these into your custom configs later). The easiest way to get the TrackML dataset is to use the Kaggle API. Install it with pip install kaggle and grab a small toy dataset with kaggle competitions download \\ -c trackml-particle-identification \\ -f train_sample.zip \\ -p $EXATRKX_DATA","title":"2. Get Dataset"},{"location":"pipelines/quickstart/#3-running-the-pipeline","text":"","title":"3. Running the Pipeline"},{"location":"pipelines/quickstart/#configuration","text":"A pipeline runs at three layers of configuration, to allow for as much flexibility as possible. To get running immediately however, you needn't change any of the defaults. From the src/Pipelines/TrackML_Example/ directory, we run python run_pipeline.py config/pipeline_quickstart.yaml which loads the pipeline specified in config/pipeline_quickstart . While it's running, get a cup of tea and a Tim-Tam, and let's see what it's doing:","title":"Configuration"},{"location":"pipelines/quickstart/#default-behaviour","text":"Our quickstart pipeline is running three stages, with a single configuration for each. You can see in config/pipeline_quickstart.yaml that the three stages are: A Processing stage with the class FeatureStore and config prepare_small_feature_store.yaml ; An Embedding stage with the class LayerlessEmbedding and config train_small_embedding.yaml ; and A Filter stage with the class VanillaFilter and config train_small_filter.yaml . The Processing stage is exactly that: data processing. It is not \"trainable\", and so the pipeline treats it differently than a trainable stage. Under the hood, it is a LightningDataModule, rather than the trainable models, which inherit from LightningModule. In this case, FeatureStore is performing some calculations on the cell information in the detector, and constructing truth graphs that will later be used for training. These calculations are computationally expensive, so it doesn't make sense to calculate them on-the-fly while training. The trainable models Embedding and Filter are learning the non-linear metric of the truth graphs, and pairwise likelihoods of hits sharing a truth graph edge, respectively. The details are not so important at this stage, what matters is that these stages are modular: Each one can be run alone, but by adding a callback to the end, it can prepare the dataset for the next stage. Looking at LightningModules/Embedding/train_small_embedding.yaml you will see that a callback is given as callbacks: EmbeddingInferenceCallback . Any number of callbacks can be added, and they adhere to the Lightning callback system . The one referred to here runs the best version of the trained Embedding model on each directory of the data split (train, val, test) and saves it for the next stage. We could also add a telemetry callback, e.g. the EmbeddingTelemetry callback in LightningModules/Embedding/Models/inference.py . This callback prints a PDF of the transverse momentum vs. the efficiency of the metric learning model, saving it in the output_dir . It \"hooks\" into the testing phase, which is run after every training phase: The default settings of this run pull from the three configuration files given at each stage. You can look at them Global config: Model library: This is in the repository Artifact library: This should be a place where you can store data = checkpoints Pipeline config: explain stages in it Explain default model config parameters (1GeV, n_train=90, n_val/n_test = 5)","title":"Default behaviour"},{"location":"tools/data/","text":"Data & Graph Manipulation","title":"Data Manipulation"},{"location":"tools/data/#data-graph-manipulation","text":"","title":"Data &amp; Graph Manipulation"},{"location":"tools/frameworks/","text":"","title":"PyLightning & MLFlow"},{"location":"tools/ml/","text":"Techniques for Training & Inference","title":"Training & Inference"},{"location":"tools/ml/#techniques-for-training-inference","text":"","title":"Techniques for Training &amp; Inference"},{"location":"tools/overview/","text":"Tools, Techniques and Best Practices Explain the rationale and aim Give examples of some tools and applications Radius-graph/knn from FAISS Edge matching Truth graph builder (Pandas building, would be nice to get this on CuPy) Triplet selection (possibly use the PyEmbedding version - benchmark) Triplet/line graph converter Checkpointing Mixed precision Remove duplicate edges Cell feature calculation Training best practices (data balancing, hard negative mining, load balancing)","title":"Overview"},{"location":"tools/overview/#tools-techniques-and-best-practices","text":"Explain the rationale and aim Give examples of some tools and applications Radius-graph/knn from FAISS Edge matching Truth graph builder (Pandas building, would be nice to get this on CuPy) Triplet selection (possibly use the PyEmbedding version - benchmark) Triplet/line graph converter Checkpointing Mixed precision Remove duplicate edges Cell feature calculation Training best practices (data balancing, hard negative mining, load balancing)","title":"Tools, Techniques and Best Practices"}]}